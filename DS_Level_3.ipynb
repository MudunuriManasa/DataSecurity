{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XwsE7LyOmwZ9",
        "outputId": "02c13661-c480-4483-e13e-f32ad16daae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global Epoch 1/20\n",
            "Malicious clients detected: []\n",
            "Test Accuracy after round 1: 93.64%\n",
            "Global Epoch 2/20\n",
            "Malicious clients detected: [6]\n",
            "Test Accuracy after round 2: 96.29%\n",
            "Global Epoch 3/20\n",
            "Malicious clients detected: [6]\n",
            "Test Accuracy after round 3: 97.12%\n",
            "Global Epoch 4/20\n",
            "Malicious clients detected: [6]\n",
            "Test Accuracy after round 4: 97.43%\n",
            "Global Epoch 5/20\n",
            "Malicious clients detected: [6]\n",
            "Test Accuracy after round 5: 97.39%\n",
            "Global Epoch 6/20\n",
            "Malicious clients detected: [7]\n",
            "Test Accuracy after round 6: 97.36%\n",
            "Global Epoch 7/20\n",
            "Malicious clients detected: [7]\n",
            "Test Accuracy after round 7: 97.43%\n",
            "Global Epoch 8/20\n",
            "Malicious clients detected: [7]\n",
            "Test Accuracy after round 8: 97.66%\n",
            "Global Epoch 9/20\n",
            "Malicious clients detected: [7]\n",
            "Test Accuracy after round 9: 97.78%\n",
            "Global Epoch 10/20\n",
            "Malicious clients detected: [7]\n",
            "Test Accuracy after round 10: 97.79%\n",
            "Global Epoch 11/20\n",
            "Malicious clients detected: [7]\n",
            "Test Accuracy after round 11: 97.89%\n",
            "Global Epoch 12/20\n",
            "Malicious clients detected: [7]\n",
            "Test Accuracy after round 12: 97.85%\n",
            "Global Epoch 13/20\n",
            "Malicious clients detected: [7]\n",
            "Test Accuracy after round 13: 97.69%\n",
            "Global Epoch 14/20\n",
            "Malicious clients detected: [7]\n",
            "Test Accuracy after round 14: 97.84%\n",
            "Global Epoch 15/20\n",
            "Malicious clients detected: [7]\n",
            "Test Accuracy after round 15: 98.04%\n",
            "Global Epoch 16/20\n",
            "Malicious clients detected: [7]\n",
            "Test Accuracy after round 16: 97.98%\n",
            "Global Epoch 17/20\n",
            "Malicious clients detected: [7]\n",
            "Test Accuracy after round 17: 98.00%\n",
            "Global Epoch 18/20\n",
            "Malicious clients detected: [7]\n",
            "Test Accuracy after round 18: 98.07%\n",
            "Global Epoch 19/20\n",
            "Malicious clients detected: [7]\n",
            "Test Accuracy after round 19: 98.02%\n",
            "Global Epoch 20/20\n",
            "Malicious clients detected: [7]\n",
            "Test Accuracy after round 20: 98.03%\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from scipy.fft import fft2, fftshift\n",
        "from skimage.filters import sobel\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "\n",
        "# Define the CNN architecture\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 14 * 14, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Load data (each client will load its own data in a real FL scenario)\n",
        "def load_data(transform, datasets='MNIST'):\n",
        "    if datasets.upper() == 'MNIST':\n",
        "        train_dataset = torchvision.datasets.MNIST(\n",
        "            root=\"./data/mnist\", train=True, download=True, transform=transform)\n",
        "        test_dataset = torchvision.datasets.MNIST(\n",
        "            root=\"./data/mnist\", train=False, download=True, transform=transform)\n",
        "    else:\n",
        "        train_dataset = torchvision.datasets.CIFAR10(\n",
        "            root=\"./data/cifar-10-python\", train=True, download=True, transform=transform)\n",
        "        test_dataset = torchvision.datasets.CIFAR10(\n",
        "            root=\"./data/cifar-10-python\", train=False, download=True, transform=transform)\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "\n",
        "\n",
        "# Partition dataset for federated learning\n",
        "def partition_dataset(dataset, n_clients):\n",
        "    split_size = len(dataset) // n_clients\n",
        "    return random_split(dataset, [split_size] * n_clients)\n",
        "\n",
        "\n",
        "# Detection Functions\n",
        "def detect_checkerboard_or_adversarial_patterns(images):\n",
        "    flagged_indices = []\n",
        "    for idx, image in enumerate(images):\n",
        "        image_np = image.numpy()[0, :, :]\n",
        "        freq = np.abs(fftshift(fft2(image_np)))\n",
        "        freq_mean, freq_std = freq.mean(), freq.std()\n",
        "        edges = sobel(image_np)\n",
        "        edge_mean = edges.mean()\n",
        "        if freq_mean > 50 and freq_std > 30 and edge_mean > 0.1:\n",
        "            flagged_indices.append(idx)\n",
        "    return flagged_indices\n",
        "\n",
        "\n",
        "def detect_poisoned_labels(dataset):\n",
        "    labels = [label for _, label in dataset]\n",
        "    clustering = DBSCAN(eps=1, min_samples=5).fit(np.array(labels).reshape(-1, 1))\n",
        "    poisoned_indices = [\n",
        "        idx for idx, label in enumerate(labels) if clustering.labels_[idx] == -1\n",
        "    ]\n",
        "    return poisoned_indices\n",
        "\n",
        "\n",
        "def detect_noisy_inputs(images):\n",
        "    flagged_indices = []\n",
        "    for idx, image in enumerate(images):\n",
        "        image_np = image.numpy()[0, :, :]\n",
        "        variance = np.var(image_np)\n",
        "        if variance > 0.2:\n",
        "            flagged_indices.append(idx)\n",
        "    return flagged_indices\n",
        "\n",
        "\n",
        "# Preprocessing Functions\n",
        "def preprocess_images(dataset, flagged_indices):\n",
        "    new_dataset = []\n",
        "    for idx, (image, label) in enumerate(dataset):\n",
        "        if idx in flagged_indices:\n",
        "            image = torchvision.transforms.functional.gaussian_blur(image, kernel_size=(3, 3))\n",
        "        new_dataset.append((image, label))\n",
        "    return new_dataset\n",
        "\n",
        "\n",
        "def clean_labels(dataset, poisoned_indices):\n",
        "    new_dataset = []\n",
        "    for idx, (image, label) in enumerate(dataset):\n",
        "        if idx in poisoned_indices:\n",
        "            label = -1\n",
        "        new_dataset.append((image, label))\n",
        "    return new_dataset\n",
        "\n",
        "\n",
        "# Simulate Malicious Client Updates\n",
        "def simulate_malicious_client_update(global_model):\n",
        "    \"\"\"\n",
        "    Simulate a malicious client by submitting random updates.\n",
        "    \"\"\"\n",
        "    malicious_model = copy.deepcopy(global_model)\n",
        "    for param in malicious_model.parameters():\n",
        "        param.data = torch.rand_like(param)  # Randomized parameters\n",
        "    return malicious_model\n",
        "\n",
        "\n",
        "# Detect Malicious Clients\n",
        "def detect_malicious_clients(global_model, client_models, threshold=2.0):\n",
        "    \"\"\"\n",
        "    Detect malicious clients using L2 norm outlier detection.\n",
        "    \"\"\"\n",
        "    global_state = global_model.state_dict()\n",
        "    distances = []\n",
        "\n",
        "    for client_model in client_models:\n",
        "        if client_model is None:\n",
        "            distances.append(float('inf'))  # Assign a large distance for safety\n",
        "            continue\n",
        "\n",
        "        client_state = client_model.state_dict()\n",
        "        dist = sum(\n",
        "            (torch.norm(global_state[key] - client_state[key]).item())\n",
        "            for key in global_state.keys()\n",
        "        )\n",
        "        distances.append(dist)\n",
        "\n",
        "    mean_dist = np.mean(distances)\n",
        "    std_dist = np.std(distances)\n",
        "\n",
        "    if std_dist == 0:  # Handle case where all distances are identical\n",
        "        std_dist = 1e-6  # Avoid division by zero\n",
        "\n",
        "    # Flag clients with distances greater than mean + threshold * std\n",
        "    malicious_clients = [\n",
        "        idx for idx, dist in enumerate(distances) if dist > mean_dist + threshold * std_dist\n",
        "    ]\n",
        "    return malicious_clients\n",
        "\n",
        "\n",
        "# Aggregate Models\n",
        "def server_aggregate(global_model, client_models, malicious_clients):\n",
        "    \"\"\"\n",
        "    Aggregate model weights from clients into the global model, excluding malicious clients.\n",
        "    \"\"\"\n",
        "    global_state = global_model.state_dict()\n",
        "    valid_states = [\n",
        "        client_model.state_dict()\n",
        "        for idx, client_model in enumerate(client_models)\n",
        "        if idx not in malicious_clients and client_model is not None\n",
        "    ]\n",
        "\n",
        "    for k in global_state.keys():\n",
        "        client_updates = torch.stack([state[k] for state in valid_states], dim=0)\n",
        "        global_state[k] = torch.mean(client_updates, dim=0)\n",
        "\n",
        "    global_model.load_state_dict(global_state)\n",
        "\n",
        "\n",
        "# Federated Learning\n",
        "def federated_learning(n_clients, global_epochs, local_epochs, malicious_client_idx, attack_start_round):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "    train_dataset, test_dataset = load_data(transform)\n",
        "    client_datasets = partition_dataset(train_dataset, n_clients)\n",
        "    client_loaders = [DataLoader(dataset, batch_size=64, shuffle=True) for dataset in client_datasets]\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "    global_model = ConvNet().to(device)\n",
        "    client_models = [copy.deepcopy(global_model) for _ in range(n_clients)]\n",
        "    optimizers = [torch.optim.Adam(client_model.parameters(), lr=0.001) for client_model in client_models]\n",
        "\n",
        "    for global_epoch in range(global_epochs):\n",
        "        print(f\"Global Epoch {global_epoch + 1}/{global_epochs}\")\n",
        "\n",
        "        for client_idx in range(n_clients):\n",
        "            if global_epoch >= attack_start_round and client_idx == malicious_client_idx:\n",
        "                client_models[client_idx] = simulate_malicious_client_update(global_model)\n",
        "            else:\n",
        "                # Train each client's model\n",
        "                for data, labels in client_loaders[client_idx]:\n",
        "                    data, labels = data.to(device), labels.to(device)\n",
        "                    optimizer = optimizers[client_idx]\n",
        "                    optimizer.zero_grad()\n",
        "                    output = client_models[client_idx](data)\n",
        "                    loss = F.cross_entropy(output, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "        # Detect and mitigate malicious clients\n",
        "        malicious_clients = detect_malicious_clients(global_model, client_models)\n",
        "        print(f\"Malicious clients detected: {malicious_clients}\")\n",
        "\n",
        "        server_aggregate(global_model, client_models, malicious_clients)\n",
        "\n",
        "        # Evaluate the global model\n",
        "        accuracy = evaluate_model(global_model, test_loader, device)\n",
        "        print(f\"Test Accuracy after round {global_epoch + 1}: {accuracy:.2f}%\")\n",
        "\n",
        "    torch.save(global_model.state_dict(), \"robust_federated_model.pth\")\n",
        "\n",
        "\n",
        "# Model Evaluation\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for data, labels in test_loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "# Main Function\n",
        "if __name__ == \"__main__\":\n",
        "    federated_learning(\n",
        "        n_clients=10, global_epochs=20, local_epochs=2, malicious_client_idx=7, attack_start_round=5\n",
        "    )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/robust_federated_model.pth')\n"
      ],
      "metadata": {
        "id": "JxxdoYep8P7K",
        "outputId": "5c43f66a-543f-4141-b9f9-8a4db9ccee50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c17dd1f9-aadb-4d2f-a495-4377f855d8b7\", \"robust_federated_model.pth\", 6506932)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WSHdsWtdLX-l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}