{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElOoV6FiRxIs"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Set environment variable for duplicate library error\n",
        "os.environ['KMP_DUPLICATE_LIB_OK'] = \"TRUE\"\n",
        "\n",
        "# Set hyperparameters\n",
        "n_clients = 10\n",
        "epochs = 10  # total epochs\n",
        "local_epochs = 1  # local epochs of each user at an iteration\n",
        "lr = 3e-3  # learning rate\n",
        "cudaIdx = \"cuda:0\"  # GPU card index\n",
        "device = torch.device(cudaIdx if torch.cuda.is_available() else \"cpu\")\n",
        "num_workers = 30  # workers for dataloader\n",
        "\n",
        "\n",
        "# Class to sample equal number of users in each training round\n",
        "class EqualUserSampler(object):\n",
        "    def __init__(self, n, num_users) -> None:\n",
        "        self.i = 0\n",
        "        self.selected = n\n",
        "        self.num_users = num_users\n",
        "        self.get_order()\n",
        "\n",
        "    def get_order(self):\n",
        "        self.users = np.arange(self.num_users)\n",
        "\n",
        "    def get_useridx(self):\n",
        "        selection = []\n",
        "        for _ in range(self.selected):\n",
        "            selection.append(self.users[self.i])\n",
        "            self.i += 1\n",
        "            if self.i >= self.num_users:\n",
        "                self.get_order()\n",
        "                self.i = 0\n",
        "        return selection\n",
        "\n",
        "\n",
        "# CNN model definition\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        # self.conv1 = nn.Conv2d(1, 6, 5)  # Old convolution layer\n",
        "        self.conv1 = nn.Conv2d(1, 32, 5)  # Increased filters for better feature extraction\n",
        "        self.bn1 = nn.BatchNorm2d(32)  # Added batch normalization\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        # self.conv2 = nn.Conv2d(6, 16, 5)  # Old second convolution layer\n",
        "        self.conv2 = nn.Conv2d(32, 64, 5)  # Increased filters in second convolution layer\n",
        "        self.bn2 = nn.BatchNorm2d(64)  # Added batch normalization\n",
        "        # Added dropout for regularization\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        # self.fc1 = nn.Linear(16 * 4 * 4, 120)  # Old fully connected layer\n",
        "        self.fc1 = nn.Linear(64 * 4 * 4, 120)  # Updated based on increased filters\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "        self.relu = nn.ReLU()  # Kept ReLU activation as-is\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.pool(self.relu(self.conv1(x)))  # Old forward pass\n",
        "        x = self.pool(self.relu(self.bn1(self.conv1(x))))  # New forward pass with batch norm\n",
        "        # x = self.pool(self.relu(self.conv2(x)))  # Old second layer forward pass\n",
        "        x = self.pool(self.relu(self.bn2(self.conv2(x))))  # New second layer with batch norm\n",
        "        x = x.view(-1, 64 * 4 * 4)  # Adjusted dimensions for increased filters\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(self.relu(self.fc2(x)))  # Added dropout after second fully connected layer\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Data transformation with augmentation\n",
        "transform = transforms.Compose([\n",
        "    # transforms.ToTensor(),  # Old transformation\n",
        "    transforms.RandomRotation(10),  # Random rotation for augmentation\n",
        "    transforms.RandomAffine(0, translate=(0.1, 0.1)),  # Random translation for augmentation\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "\n",
        "# Load data (each client will load its own data in a real FL scenario)\n",
        "def load_data(transform, datasets='MNIST'):\n",
        "    if datasets.upper() == 'MNIST':\n",
        "        train_dataset = torchvision.datasets.MNIST(\n",
        "            root=\"./data/mnist\", train=True, download=True, transform=transform)\n",
        "        test_dataset = torchvision.datasets.MNIST(\n",
        "            root=\"./data/mnist\", train=False, download=True, transform=transform)\n",
        "    else:\n",
        "        train_dataset = torchvision.datasets.CIFAR10(\n",
        "            root=\"./data/cifar-10-python\", train=True, download=True, transform=transform)\n",
        "        test_dataset = torchvision.datasets.CIFAR10(\n",
        "            root=\"./data/cifar-10-python\", train=False, download=True, transform=transform)\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "\n",
        "# Partition the dataset into 'n_clients' partitions\n",
        "def partition_dataset(dataset, n_clients):\n",
        "    split_size = len(dataset) // n_clients\n",
        "    return random_split(dataset, [split_size] * n_clients)\n",
        "\n",
        "\n",
        "# FedAvgServer class for managing global aggregation\n",
        "class FedAvgServer:\n",
        "    def __init__(self, global_parameters):\n",
        "        self.global_parameters = global_parameters\n",
        "\n",
        "    def download(self, user_idx):\n",
        "        local_parameters = []\n",
        "        for i in range(len(user_idx)):\n",
        "            local_parameters.append(copy.deepcopy(self.global_parameters))\n",
        "        return local_parameters\n",
        "\n",
        "    def upload(self, local_parameters):\n",
        "        for k, v in self.global_parameters.items():\n",
        "            tmp_v = torch.zeros_like(v)\n",
        "            for j in range(len(local_parameters)):\n",
        "                tmp_v += local_parameters[j][k]\n",
        "            tmp_v = tmp_v / len(local_parameters)  # FedAvg\n",
        "            self.global_parameters[k] = tmp_v\n",
        "\n",
        "\n",
        "# Client class for local training\n",
        "class Client:\n",
        "    def __init__(self, data_loader, user_idx):\n",
        "        self.data_loader = data_loader\n",
        "        self.user_idx = user_idx\n",
        "\n",
        "    def train(self, model, learningRate, idx, global_model):\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)  # Added scheduler\n",
        "        for epoch in range(epochs):\n",
        "            for data, labels in self.data_loader:\n",
        "                data, labels = data.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                output = model(data)\n",
        "                loss = F.cross_entropy(output, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            scheduler.step()  # Step scheduler at the end of the epoch\n",
        "\n",
        "\n",
        "# Activate clients\n",
        "def activateClient(train_dataloaders, user_idx, server):\n",
        "    local_parameters = server.download(user_idx)\n",
        "    clients = [Client(train_dataloaders[user], user) for user in user_idx]\n",
        "    return clients, local_parameters\n",
        "\n",
        "\n",
        "# Train function for orchestrating client training\n",
        "def train(train_dataloaders, user_idx, server, global_model, learningRate):\n",
        "    clients, local_parameters = activateClient(train_dataloaders, user_idx, server)\n",
        "    for i in range(len(user_idx)):\n",
        "        model = ConvNet().to(device)\n",
        "        model.load_state_dict(local_parameters[i])\n",
        "        model.train()\n",
        "        clients[i].train(model, learningRate, i, global_model)\n",
        "        local_parameters[i] = model.state_dict()\n",
        "    server.upload(local_parameters)\n",
        "    global_model.load_state_dict(server.global_parameters)\n",
        "\n",
        "\n",
        "# Test function with additional metrics\n",
        "'''\n",
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for data, labels in test_loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            output = model(data)\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    # Print evaluation metrics\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred))\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(y_true, y_pred))\n",
        "    '''\n",
        "\n",
        "def test(model, test_loader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for data, labels in test_loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            output = model(data)\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "def train_main(n_clients=10):\n",
        "    global_model = ConvNet().to(device)\n",
        "    global_parameters = global_model.state_dict()\n",
        "    server = FedAvgServer(global_parameters)\n",
        "\n",
        "    train_dataset, test_dataset = load_data(transform)\n",
        "    client_datasets = partition_dataset(train_dataset, n_clients)\n",
        "    client_loaders = [DataLoader(dataset, batch_size=50, shuffle=True, num_workers=num_workers)\n",
        "                      for dataset in client_datasets]\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "    sampler = EqualUserSampler(n_clients, n_clients)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        print(f'Global Epoch {epoch}/{epochs}')\n",
        "        user_idx = sampler.get_useridx()\n",
        "\n",
        "        train(client_loaders, user_idx, server, global_model, lr)\n",
        "\n",
        "         # Evaluate global model on test dataset\n",
        "        test_accuracy = test(global_model, test_loader, device)\n",
        "        print(\n",
        "            f'Global Model Test Accuracy after round {epoch}: {test_accuracy:.4f}')\n",
        "\n",
        "        # Evaluate global model on test dataset\n",
        "       # evaluate_model(global_model, test_loader, device)\n",
        "\n",
        "    # Save the final global model\n",
        "    torch.save(global_model.state_dict(), 'federated_model.pth')\n",
        "    print(\"Federated learning process completed.\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train_main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}